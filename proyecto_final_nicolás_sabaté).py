# -*- coding: utf-8 -*-
"""Proyecto FInal  - Nicolás Sabaté)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ZkGbKk2Ho7kuoy-AYTs1XlAsV2weres

**ABSTRACT**

Utilizaremos el dataset "Churn_Modelling" para analizar que variables influyen principalmente en la decisión de los clientes de abandonar el banco del que proviene nuestro dataset.

El dataset describe a los clientes de banco con sus respectivas caracteristicas tales como la edad, el salario estimado, el sexo , el balance de su cuenta y la cantidad de productos que posee del banco.

A partir de estos datos, buscaremos identificar donde se encuentran las variables que más influyen en la salida de los clientes del banco a través del diseño y ejecución de distintos tipos de gráficos exploratorios para sacar insights o relaciones entre las distintas variables.

Ejecutaremos gráficos que relacionen las distintas variables, utilizaremos gráficos de líneas , gráficos de barras y gráficos de dispersión según el tipo de datos con el que estemos tratando.

A mediado que avancemos en las visualizaciones, dejaremos explicito los insights conseguidos y finalizaremos con una conclusión de los datos que pudimos obtener.

**Objetivo**

* Obtener insights a través de la realización de un EDA (Exploratory Data Analysis) para determinar cuales son nuestras variables más importantes y conocer nuestro dataset.
* Aplicar algún algoritmo de Machine Learning que nos permita predecir las causas o que variables influyen más en la pérdida de clientes del banco analizado.
* Dar posibles líneas de solución o cambios ante el problema analizado.

**Contexto**

El banco necesita saber porque razón parte de sus clientes están abandonando el banco y se están yendo con la competencia. Por esa razón necesita investigar las razones y los patrones de sus clientes para ajustar la estrategia de la empresa y poder retener a sus actuales clientes, recuperar algunos de los que se fueron del banco y sumar a nuevos clientes.

**Problema comercial**
El banco está perdiendo mucho clientes y necesita saber que está ocurriendo y ver que es lo que está haciendo a los clientes abandonar su banco, ya sea los ingresos que tienen, el pais, la edad o cualquier otra variable que pueda influir. El Banco es el principal beneficiario de este análisis ya que va a poder entender las razones por las cúales está perdiendo clientes.Determinar las condiciones y factores que influyen en el "churn" es clave para redirigir la estrategia de la entidad bancaria.

**Pregunta/Hipótesis**

Por que razón el banco piede clientes? ¿Influye la cantidad de productos que poseen los clientes, su balance o hay otra variable que explique la salida de clientes? 

* Esperamos una correlación entre las personas que abandonan el banco y las que menor cantidad de dinero poseen.
* Esperamos que las personas con menor Credit Score son las que menos dinero ganan. 
* Esperamos que la gente de menor edad, sea la que tiene menor poder adquisitivo.

# **EDA (Exploratory Data Analysis)**
"""

#Importamos las librerías necesarias para arrancar
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt

#Cargamos nuestro dataset
db = pd.read_csv("Churn_Modelling.csv", sep =",")

db.head(10)

db.shape

#Vemos que información podemos sacar de nuestras variables númericas
db.describe()

"""* Nuestro dataset contiene 10.000 filas y 14 columnas y se basa en la base de datos de clientes de un banco con las respectivas características de lo clientes.

* El Credit Score medio es de 650 , mientras que el mínimo es de 350 y el máximo de 850.

* La edad promedio de los clientes es de 38 años mientras que su balance promedio es de 76.485 dólares redondeado. Su salario promedio estimado es de 100.090 dólares.

* Por último, cabe destacar que el banco posee 4 productos, y la mayoria tiene en promedio 1.5% , en es decir entre 1 y 2 productos. 
"""

#Vemos si hay algo interesante en nuestras variables categóricas
db.describe(include= ["object", "bool"])

"""* El Banco tiene clientes de 3 países, siendo Francia el país con más clientes con 5014.
* El apellido más frecuente en los clientes es Smith, con 32 personas que se apellidan de esta forma.
"""

db.Age.mean

print(db["Geography"].value_counts(normalize= True)* 100)
print(db["Geography"].value_counts())

print(db["Gender"].value_counts(normalize= True)* 100)
print(db["Gender"].value_counts())

print(db["Exited"].value_counts(normalize= True)* 100)
print(db["Exited"].value_counts())

db["NumOfProducts"].value_counts(normalize= True)* 100

"""# **Gráficos exploratorios**"""

db.columns

sns.violinplot(db.EstimatedSalary, orient='v')

"""* El salario se encuentra distribuido de manera pareja en todo el dataset"""

#Realizamos un scatterplot para ver si la edad influye en los ingresos
sns.set(rc={'figure.figsize':(11.7,8.27)})
scatter= sns.regplot(data=db, x= "Age", y= "EstimatedSalary")
#Con esta función podemos ponerle limites a los gráficos scatter.set_ylim(0, 1000)

"""No se observan relaciones entre el salario estimado y la edad. No se observa una relación ascendente ni descendente significativa"""

#Utilizando gráfico de barras y agrupando por categoría.
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.displot(data=db, x= "NumOfProducts", hue="NumOfProducts", bins=4, multiple= "stack", col= "Exited")

"""La mayoría de los clientes tiene un producto solo mientras que otra gran cantidad posee 2 productos. Pocos clientes poseen 3 productos o 4. """

#Utilizando gráfico de barras y agrupando por categoría.
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.displot(data=db, x= "Gender",hue= "Geography", multiple= "stack", col= "Geography")

"""De este gráfico obtenemos tres insights principales:
* Hay más clientes hombres que mujeres en todos los países y en general.
* El país con más clientes es Francia.
* La brecha de género en materia de clientes es menor en Alemania.
"""

sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.boxplot(data= db, x="CreditScore", y= "Geography" , hue="Geography")

"""* El Credit score usual va de 400 a 850. Debajo de esos números encontramos nuestros ouliers o valores atipicos. No se observan fuertes diferencias entre el CreditScore de los distintos países."""

#Utilizando gráfico de barras y agrupando por categoría.
sns.set(rc={'figure.figsize':(20,16)})
sns.displot(data=db, x= "Exited",hue= "Geography", multiple= "stack", col= "Geography", bins=2)

"""**INSIGHTS PRELIMINARES**

* El Credit Score medio es de 650 , mientras que el mínimo es de 350 y el máximo de 850. Nuestro EDA tomó como Outliers a los que poseen un crdit score menor a 400. No se observan distinciones entre los distintos países.

* La edad promedio de los clientes es de 38 años mientras que su balance promedio es de 76.485 dólares redondeado. Su salario promedio estimado es de 100.090 dólares anuales.

* El Banco tiene clientes de 3 países, siendo Francia el país con más clientes con 5014 seguido de Alemania con 2509 y España con 2479. El apellido más frecuente en los clientes es Smith, con 32 personas que se apellidan de esta forma. 5457 clientes son hombres y 4543 son mujeres (54.6% y 45.4% respectivamente).

* El banco posee 4 productos, y la mayoria tiene en promedio 1.5% , en es decir entre 1 y 2 productos.

* 2037 clientes dejaron el banco por lo que es esencial analizar a este grupo para ver que diferencia tiene con los demás.

* El problema del churn tiene algunos insights interesantes. La variable "Exited" se ve fortalecida en algunas opciones de la variable "Geography" como Alemania y en otras opciones de la variable "NumOfProducts" como 1.

# **Conversión de variables**
"""

#Importamos la librería para procesar la data. sklearn es la librería principal de ML
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
from sklearn.metrics import roc_curve, auc #Se importan metricas
from sklearn.model_selection import StratifiedKFold

import string
import warnings
warnings.filterwarnings('ignore')

#Consultamos los tipos de variables que tenemos para comenzar
db.info()

"""* La mayoría de nuestras varibales están en formato int o float lo que nos facilita el procesamiento. 
* Las variables Geography y Gender deben ser transformadas a través de One Hot Encoder 
* Dropearemos la variable surname, teniendo de referencia el Id.
"""

db.head(5)

db["Surname"].nunique()

#Dropeamos la categoría Surname ya que no nos sirve y nos dificulta el algoritmo
db.drop("Surname", axis=1, inplace= True)

#Tambien dropearemos la columna de CustomerId por sus numeros tan altos, nos guiaremos por "RowNumber"
db.drop("CustomerId", axis=1, inplace= True)

"""**Debemos transformar las variables Gender y Geography a través de OneHotEncoding**

* Las variables no tienen jerarquía
* Las variables tienen pocas categorías


"""

#Creamos con la variable Gender y Geography , más variables utilizando ONE HOT ENCONDING
db= pd.get_dummies(db, prefix = ["Gender"], columns=["Gender"])
db= pd.get_dummies(db, prefix = ["Geography"], columns=["Geography"])

db.head(10)



"""* Las nuevas columnas fueron creadas correctamente.
* Ahora debemos chequear que todas nuestras variables sean númericas para poder ejercitar nuestro algoritmo.
"""

db.info()

#Le cambiamos el nombre a la columna RowNumber para identificarla mejor
db.rename(columns=({ 'RowNumber': 'Id'}), inplace=True)

#Analizamos la correlaciones entre las variables
db.corr()

correlations= db.corr()

#Utilizamos un mapa de calor para visualizar las correlaciones
indx=correlations.index 
plt.figure(figsize=(16,12))
sns.heatmap(db[indx].corr(),annot=True,cmap="YlGnBu")

"""# **Funciones para tratar con columnas**

**Group By**
"""

#Aplicamos el group by para después pedirle resultados
grouped=db['CreditScore'].groupby(db['NumOfProducts'])
type(grouped)

#Obtenemos la media de Credit Score por cantidad de productos
grouped.mean()

"""**Apply**"""

#Copio mi Database
db1= db.copy()
db1

# Creo la función que deseo aplicar
def Multiplicar(db1):
  return db1*2

db1.apply(Multiplicar)

"""**Pivot**"""

#Hago un pivot con la tabla para que verla de otra forma
db1.pivot(index='Id',columns='NumOfProducts')

"""**Melt**"""

db2 = db.copy()

#Cambio el formato de la tabla para que se observe su nombre y su valor de forma consecutiva
pd.melt(db2, id_vars=['Id'], value_vars=['CreditScore'], var_name='Nombre_Variable', value_name='Valor')

"""# **Seleccionando y entrenando el algoritmo**

* Realizar un DesicionTree con Exited como variable target.
* No se puede realizar una regresión ya que nuestra variable target es una variable booleana que va entre 0 y 1. No es continua.
"""

#La principal librería para Machine Learning
from sklearn.linear_model import LinearRegression

from sklearn.tree import DecisionTreeClassifier #estoy importando el algoritmo de Arboles de decision

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from sklearn.model_selection import train_test_split # Import train_test_split function

from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

#Definimos las variables que vamos a utilizar para la regresión
X = db.drop('Exited', axis=1)# Todas menos Exited - nuestra variable target (Y)
y = db.Exited #Variable target

#Dividimos el algoritmo en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=10)

#definir mi modelo
modelo_dt = DecisionTreeClassifier(max_depth = 5, criterion ='entropy')

#entrenarl el modelo
modelo_dt.fit(X_train, y_train)

#pedirle predicciones
y_pred_dt = modelo_dt.predict(X_test)

result = confusion_matrix(y_test, y_pred_dt)
print("Confusion Matrix:")
print(result)

result1 = classification_report(y_test, y_pred_dt)
print("Classification Report:",)
print (result1)

result2 = accuracy_score(y_test,y_pred_dt)
print("Accuracy:",result2)

#Veo que variables tienen mayor capacidad predictiva
modelo_dt.feature_importances_

#Te dice que variables sirvieron para predecir y cuales no
feature_imp = pd.Series(modelo_dt.feature_importances_,index=X_train.columns).sort_values(ascending=False)
feature_imp

# Creamos un barplot para ve la capacidad de cada variable
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

"""# 


"""

#Realizamos un mapa de predicción del modelo
plt.figure(figsize=(10,6))
metrics.plot_confusion_matrix(modelo_dt, X_test, y_test, display_labels=['Negative', 'Positive'])

confusion = metrics.confusion_matrix(y_test, y_pred_dt)
confusion.ravel()

#Medimos la accurracy del modelo
accuracy = metrics.accuracy_score(y_test, y_pred_dt)
accuracy

# Precision - Se evalua para cada categoria
precision_positiva = metrics.precision_score(y_test, y_pred_dt, pos_label=1)
precision_negativa = metrics.precision_score(y_test, y_pred_dt, pos_label=0)
precision_positiva, precision_negativa

#Recall and Especificity
recall_sensibilidad = metrics.recall_score(y_test, y_pred_dt, pos_label=1)
recall_especificidad= metrics.recall_score(y_test, y_pred_dt, pos_label=0)
recall_sensibilidad, recall_especificidad

# Todas las metricas en uno
print(metrics.classification_report(y_test, y_pred_dt))

"""**PRESICION**: La precisión positiva es del 67.9% mientras que la negativa es del 87.2%. En general , el modelo tiene una precisión del 83% lo cual es bastante aceptable en un principio.

**ACURRACY**: La exactitud del modelo es del 84%. Es decir, nuestro modelo ha clasificado al 84% de los casos bien. 

**RECALL**: La sensibilidad del modelo es del 84%,es decir identifica correctamente a esa cantidad de casos que permanecieron en el banco.

**ESPECIFICITY**: La especificidad del modelo es del 46%,es decir identifica correctamente a esa cantidad de casos que se fuerondel banco. Es nuestra peor métrica en materia de resultados y es la que más necesitamos para predecir las salidas de los clientes del banco.

**CONCLUSIONES**

* La edad es el principal factor que influye en la salida del banco de los clientes.La cantidad de productos que se posee también es importante. Por último, el Balance de su cuenta bancaria es la tercer variable en importancia.
* En cuanto a las métricas del algoritmo, detectamos que obtenemos números aceptables en general salvo en la especificidad del modelo que obtiene un 46%, bastante bajo, por lo que habría que revisar en que se puede mejorar el modelo o si se deben categorizar de otra manera los datos. 

* Obtenemos muy buenos resultados para clasificar a las clase 0 que corresponde a los que no se fueron del banco pero una baja precisión y bajo recall para los de clase 1 que corresponden a los que se fueron del banco. 

* **Es posible que nuestro modelo este sufriendo OVERFITTING** por la cantidad de variables que no tienen incidencia predictiva que están presentes en el modelo y necesitemos simplicarlo. O **UNDERFITTING** por la poca cantidad de datos de la variable "Exited"
"""

